import time
import json
import re
import sys
import argparse
from datetime import datetime, timedelta
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.edge.service import Service as EdgeService
from webdriver_manager.microsoft import EdgeChromiumDriverManager

class Tee:
    def __init__(self, *files):
        self.files = files
    def write(self, obj):
        for f in self.files:
            f.write(obj)
            f.flush()
    def flush(self):
        for f in self.files:
            f.flush()

def setup_logging(debug_mode):
    if debug_mode:
        log_time = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_filename = f"output_log_{log_time}.txt"
        sys.stdout = Tee(sys.stdout, open(log_filename, 'a', encoding='utf-8'))

def load_cookies_from_file(filename):
    cookies = []
    with open(filename, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line.startswith("#") or not line:
                continue
            parts = line.split("\t")
            if len(parts) != 7:
                continue
            domain, flag, path, secure, expiration, name, value = parts
            cookie = {
                "domain": domain,
                "name": name,
                "value": value,
                "path": path,
                "secure": secure.upper() == "TRUE",
                "expiry": int(expiration) if expiration.isdigit() else None,
            }
            cookies.append(cookie)
    return cookies

def parse_time(text, today_str):
    today = datetime.strptime(today_str, "%Y-%m-%d")

    def parse_hour(period, hour, minute):
        hour, minute = int(hour), int(minute)
        if period in ("ä¸‹åˆ", "æ™šä¸Š") and hour != 12:
            hour += 12
        elif period == "å‡Œæ™¨":
            hour = 0 if hour == 12 else hour
        elif period == "ä¸Šåˆ" and hour == 12:
            hour = 0
        elif period == "æ¸…æ™¨":
            # æ¸…æ™¨é€šå¸¸æŒ‡ 5~7 é»ï¼Œé€™è£¡ç›´æ¥ç”¨åŸå§‹ hour
            pass
        return hour, minute

    # 1. 2023å¹´3æœˆ5æ—¥ ä¸Šåˆ10:20
    m = re.match(r"(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥\s+(å‡Œæ™¨|æ¸…æ™¨|ä¸Šåˆ|ä¸‹åˆ|æ™šä¸Š)(\d{1,2}):(\d{2})", text)
    if m:
        year, month, day, period, hour, minute = m.groups()
        hour, minute = parse_hour(period, hour, minute)
        dt = datetime(int(year), int(month), int(day), hour, minute)
        return dt.isoformat() + "Z"

    # 2. 3æœˆ5æ—¥ ä¸Šåˆ10:20
    m = re.match(r"(\d{1,2})æœˆ(\d{1,2})æ—¥\s+(å‡Œæ™¨|æ¸…æ™¨|ä¸Šåˆ|ä¸‹åˆ|æ™šä¸Š)(\d{1,2}):(\d{2})", text)
    if m:
        month, day, period, hour, minute = m.groups()
        hour, minute = parse_hour(period, hour, minute)
        dt = datetime(today.year, int(month), int(day), hour, minute)
        if dt > today:
            dt = dt.replace(year=today.year - 1)
        return dt.isoformat() + "Z"

    # 3. ä»Šå¤©/æ˜¨å¤© ä¸Šåˆ10:20
    if text.startswith("ä»Šå¤© ") or text.startswith("æ˜¨å¤© "):
        day_part, time_part = text.split()
        base_date = today if day_part == "ä»Šå¤©" else today - timedelta(days=1)
        m = re.match(r"(å‡Œæ™¨|æ¸…æ™¨|ä¸Šåˆ|ä¸‹åˆ|æ™šä¸Š)(\d{1,2}):(\d{2})", time_part)
        if not m:
            return None
        period, hour, minute = m.groups()
        hour, minute = parse_hour(period, hour, minute)
        return base_date.replace(hour=hour, minute=minute).isoformat() + "Z"

    return None

def scroll_one_step_to_bottom(driver, pause=2):
    last_height = driver.execute_script("return document.documentElement.scrollHeight")
    driver.execute_script("window.scrollTo(0, document.documentElement.scrollHeight);")
    time.sleep(pause)
    new_height = driver.execute_script("return document.documentElement.scrollHeight")
    return new_height > last_height

def date_to_timestamp(date_str):
    dt = datetime.strptime(date_str, "%Y/%m/%d")
    end_of_day = dt.replace(hour=23, minute=59, second=59, microsecond=999000)
    return int(end_of_day.timestamp() * 1e6)

def get_youtube_history_url(date_str=None):
    base_url = "https://myactivity.google.com/product/youtube?restrict=youtube"
    if date_str:
        max_timestamp = date_to_timestamp(date_str)
        return f"{base_url}&max={max_timestamp}"
    return base_url

def main(start_date=None, end_date=None, output_file="youtube_watch_history.json"):
    start_time = time.time()
    cookies = load_cookies_from_file("myactivity.google.com_cookies.txt")
    today_str = datetime.today().strftime("%Y-%m-%d")
    end_dt = None
    if end_date:
        end_dt = datetime.strptime(end_date, "%Y/%m/%d")

    options = webdriver.EdgeOptions()
    options.add_argument("--start-maximized")
    driver = webdriver.Edge(service=EdgeService(EdgeChromiumDriverManager().install()), options=options)

    # start_date å°æ‡‰ max= åƒæ•¸ï¼Œend_date ç‚ºçµæŸæ¢ä»¶
    url = get_youtube_history_url(start_date)
    driver.get(url)
    for cookie in cookies:
        try:
            driver.add_cookie(cookie)
        except:
            pass
    driver.refresh()
    time.sleep(5)
    for i in range(5, 0, -1):
        print(f"\rè«‹ç¢ºèªCookieç™»å…¥æˆåŠŸèˆ‡å¦ï¼Œæœƒåœ¨ {i} ç§’å¾Œç¹¼çºŒåŸ·è¡Œ...", end="", flush=True)
        time.sleep(1)
    print()  # æ›è¡Œï¼Œé¿å…è¦†è“‹å¾ŒçºŒ log

    seen_headers = set()      # æ—¥æœŸ header å¡æ± 
    seen_search_urls = set()  # æœå°‹æ´»å‹• url å¡æ± 
    seen_unique_ids = set()   # æ´»å‹•å¡ç‰‡ unique_id å¡æ± 
    seen_logs = set()
    results = []

    MAX_SCROLL_ROUNDS = float('inf')
    EMPTY_ROUND_THRESHOLD = 3
    rounds = 0
    empty_rounds = 0
    last_header = None
    last_processed_idx = 0  # æ–°å¢ï¼šè¨˜éŒ„ä¸Šä¸€è¼ªè™•ç†åˆ°çš„ index

    while rounds < MAX_SCROLL_ROUNDS:
        print(f"\n[SCROLL] ç¬¬ {rounds + 1} æ¬¡æ²å‹• ğŸ”½")
        for _ in range(2):
            scroll_one_step_to_bottom(driver, pause=5)

        # æœç´¢æ‰€æœ‰ c-wiz[class*='xDtZAf'] ä»¥æ¶µè“‹æ‰€æœ‰æ´»å‹•å¡ç‰‡
        activities = driver.find_elements(By.CSS_SELECTOR, "c-wiz.xDtZAf, div.CW0isc")
        print(f"[INFO] æ´»å‹•+æ—¥æœŸå€å¡Šç¸½æ•¸é‡ï¼š{len(activities)}")
        elapsed = time.time() - start_time
        print(f"[INFO] ç¨‹å¼å·²åŸ·è¡Œ {elapsed:.1f} ç§’")
        new_found = 0

        skip_count = 0
        processed_count = 0
        for idx, act in enumerate(activities):
            if idx < last_processed_idx:
                skip_count += 1
                print(f"\r[LOG] index={idx+1}/{len(activities)} | è™•ç†: {processed_count} | skip: {skip_count}", end="", flush=True)
                continue  # è·³éå‰é¢å·²è™•ç†éçš„
            processed_count += 1
            print(f"\r[LOG] index={idx+1}/{len(activities)} | è™•ç†: {processed_count} | skip: {skip_count}", end="", flush=True)
            try:
                # LOG 
                headers = act.find_elements(By.CSS_SELECTOR, "div.MCZgpb > h2.rp10kf")
                if headers:
                    latest = headers[-1].text.strip()
                    if latest in seen_headers:
                        continue
                    seen_headers.add(latest)
                    if latest and latest != last_header:
                        last_header = latest
                        print(f"[ğŸ“… æ—¥æœŸæ›´æ–°] ç¾åœ¨ä½¿ç”¨ï¼š{last_header}")
                        # æª¢æŸ¥æ—¥æœŸæ˜¯å¦æ—©æ–¼ end_dateï¼Œè‹¥æ˜¯å‰‡çµ‚æ­¢
                        if end_dt:
                            try:
                                # æ”¯æ´æœ‰å¹´ä»½ã€ç„¡å¹´ä»½ã€ä»Šå¤©ã€æ˜¨å¤©æ ¼å¼
                                if "å¹´" in last_header:
                                    header_dt = datetime.strptime(last_header, "%Yå¹´%mæœˆ%dæ—¥")
                                elif "ä»Šå¤©" in last_header:
                                    header_dt = datetime.today()
                                elif "æ˜¨å¤©" in last_header:
                                    header_dt = datetime.today() - timedelta(days=1)
                                else:
                                    header_dt = datetime.strptime(f"{datetime.today().year}å¹´" + last_header, "%Yå¹´%mæœˆ%dæ—¥")
                                if header_dt < end_dt:
                                    print(f"[STOP] å·²é”çµæŸæ—¥æœŸ {end_date}ï¼Œçµ‚æ­¢çˆ¬èŸ²")
                                    elapsed = time.time() - start_time
                                    print(f"[INFO] ç¨‹å¼ç¸½åŸ·è¡Œæ™‚é–“ï¼š{elapsed:.1f} ç§’")
                                    driver.quit()
                                    print(f"\nâœ… å…±å„²å­˜ {len(results)} ç­†æ´»å‹•è‡³ {output_file}")
                                    return
                            except Exception as e:
                                print(f"[WARN] æ—¥æœŸè§£æå¤±æ•—: {e}")
                    continue

                qtgv3c_text = act.find_element(By.CSS_SELECTOR, "div.QTGV3c").text.strip()

                # æª¢æŸ¥æ˜¯å¦ç‚ºæœå°‹æ´»å‹•
                if qtgv3c_text.startswith("æœå°‹ã€Œ"):
                    if qtgv3c_text in seen_search_urls:
                        continue
                    seen_search_urls.add(qtgv3c_text)
                    print(f"[LOG] æœå°‹æ´»å‹•ï¼š{qtgv3c_text}", flush=True)
                    continue  # ç›®å‰ä¸è™•ç†æœå°‹æ´»å‹•

                # æª¢æŸ¥æ˜¯å¦ç‚ºå·²æŸ¥çœ‹æ´»å‹•
                if qtgv3c_text.startswith("å·²æŸ¥çœ‹ã€Œ"):
                    if qtgv3c_text in seen_logs:
                        continue
                    seen_logs.add(qtgv3c_text)
                    print(f"[LOG] å·²æŸ¥çœ‹æ´»å‹•ï¼š{qtgv3c_text}", flush=True)
                    continue  # ç›®å‰ä¸è™•ç†å·²æŸ¥çœ‹æ´»å‹•

                # å–å¾—å”¯ä¸€IDï¼ˆc-wiz å…§ c-data çš„ id å±¬æ€§ï¼‰
                try:
                    cdata = act.find_element(By.CSS_SELECTOR, "c-data")
                    unique_id = cdata.get_attribute('id')
                except Exception:
                    unique_id = None
                if not unique_id:
                    continue

                # å–å¾— title èˆ‡ title_urlï¼ˆå¾ QTGV3c å…§çš„ a.l8sGWb å–å¾—ï¼‰
                qtgv3c_elem = act.find_element(By.CSS_SELECTOR, "div.QTGV3c")
                a_elem = qtgv3c_elem.find_element(By.CSS_SELECTOR, "a.l8sGWb")
                title = a_elem.text.strip()
                title_url = a_elem.get_attribute("href")

                time_text = act.find_element(By.CSS_SELECTOR, "div.H3Q9vf.XTnvW").text
                time_label = time_text.split("â€¢")[0].strip()

                full_time = f"{last_header} {time_label}"
                time_iso = parse_time(full_time, today_str)
                if not time_iso:
                    # print(f"[WARN] æ´»å‹•å¡ç‰‡ç¼ºå°‘ unique_id={unique_id}ï¼Œindex={idx+1}ï¼ŒHTMLï¼š\n{act.get_attribute('outerHTML')}")
                    continue

                subtitles = []
                try:
                    ch_elem = act.find_element(By.CSS_SELECTOR, "div.SiEggd a")
                    channel_name = ch_elem.text.strip()
                    channel_url = ch_elem.get_attribute("href")
                    subtitles.append({"name": channel_name, "url": channel_url})
                except:
                    pass

                results.append({
                    "header": "YouTube",
                    "title": title,
                    "titleUrl": title_url,
                    "subtitles": subtitles,
                    "time": time_iso,
                    "products": ["YouTube"],
                    "activityControls": ["YouTube watch history"]
                })

                # æ¯æ–°å¢ä¸€ç­†å°±å³æ™‚å¯«å…¥ json æª”æ¡ˆ
                with open(output_file, "w", encoding="utf-8") as f:
                    text = json.dumps(results, ensure_ascii=False, indent=2)
                    text = text.replace('},\n  {', '},{')
                    # è™•ç†åªæœ‰ä¸€å€‹å…ƒç´ çš„é™£åˆ—ï¼ˆsubtitles/products/activityControlsç­‰ï¼‰å£“æˆä¸€è¡Œï¼Œé™£åˆ—å¾Œé¢å¯æ¥é€—è™Ÿæˆ–å³å¤§æ‹¬è™Ÿ
                    text = re.sub(r'\[\n\s+({.*?})\n\s+\](,?)', r'[\1]\2', text)
                    text = re.sub(r'\[\n\s+(".*?")\n\s+\](,?)', r'[\1]\2', text)
                    f.write(text)

                new_found += 1
                # å–å¾—é »é“åç¨±ï¼ˆè‹¥æœ‰ï¼‰
                channel_display = ""
                if subtitles and isinstance(subtitles, list) and len(subtitles) > 0:
                    channel_display = f" | é »é“ï¼š{subtitles[0]['name']}"
                print(f"[âœ“] æ–°å¢ï¼š{title} @ {time_iso}{channel_display}")

            except Exception as e:
                print(f"[ERR] æ´»å‹•è™•ç†å¤±æ•—ï¼š{e}")
                continue
        print()  # æ›è¡Œï¼Œé¿å…é€²åº¦æ¢è¦†è“‹å¾ŒçºŒ log
        last_processed_idx = len(activities)  # è¨˜éŒ„æœ¬è¼ªæœ€å¾Œä¸€ç­† index

        if new_found == 0:
            empty_rounds += 1
            print(f"[INFO] ç„¡æ–°å¢ï¼ˆ{empty_rounds}/{EMPTY_ROUND_THRESHOLD}ï¼‰")
            if empty_rounds >= EMPTY_ROUND_THRESHOLD:
                print("[STOP] ç„¡æ–°æ´»å‹•ï¼Œè‡ªå‹•çµæŸ")
                break
        else:
            empty_rounds = 0

        rounds += 1

    elapsed = time.time() - start_time
    print(f"[INFO] ç¨‹å¼ç¸½åŸ·è¡Œæ™‚é–“ï¼š{elapsed:.1f} ç§’")
    print(f"\nâœ… å…±å„²å­˜ {len(results)} ç­†æ´»å‹•è‡³ {output_file}")
    driver.quit()

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--start-date', type=str, help='æŒ‡å®š max= åƒæ•¸ (æ ¼å¼: YYYY/MM/DD)', default=None)
    parser.add_argument('--end-date', type=str, help='çµæŸæ¢ä»¶ï¼Œé‡åˆ°å°æ–¼é€™å¤©çš„æ´»å‹•å°±åœæ­¢ (æ ¼å¼: YYYY/MM/DD)', default=None)
    parser.add_argument('--output', type=str, help='è¼¸å‡ºæª”æ¡ˆåç¨±', default='youtube_watch_history.json')
    parser.add_argument('--debug', action='store_true', help='å•Ÿç”¨ debug log è¼¸å‡ºåˆ°æª”æ¡ˆ')
    args = parser.parse_args()
    setup_logging(args.debug)
    main(args.start_date, args.end_date, args.output)
